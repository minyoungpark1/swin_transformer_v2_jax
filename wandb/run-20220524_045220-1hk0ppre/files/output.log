=> creating /content/drive/MyDrive/jax_exercise/output/imagenet/swint_torch
=> creating /content/drive/MyDrive/jax_exercise/log/imagenet/swinv2_tiny/swint_torch_2022-05-24-04-52
Creating dataloaders...
Loading Swin Transformer v2 model...
Namespace(accumulation_steps=None, amp_opt_level=None, batch_size=None, cache_mode='part', cfg='./config/swint_torch.yaml', data_path=None, disable_amp=False, eval=False, name='swint_torch', opts=None, pretrained=None, resume=None, seed=304, tag=None, throughput=False, use_checkpoint=False, zip=False)
AMP_ENABLE: True
AMP_OPT_LEVEL:
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 64
  CACHE_MODE: part
  DATASET: imagenet
  DATA_PATH: /content/drive/MyDrive/jax_exercise/swin_transformer_jax/imagenette2
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 8
  PIN_MEMORY: True
  ZIP_MODE: False
EVAL_MODE: False
LOCAL_RANK: 0
LOG_DIR: /content/drive/MyDrive/jax_exercise/log
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swinv2_tiny
  NUM_CLASSES: 10
  PRETRAINED:
  RESUME:
  SWIN:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 4
    QKV_BIAS: True
    QK_SCALE: None
    WINDOW_SIZE: 7
  SWINV2:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES: [0, 0, 0, 0]
    QKV_BIAS: True
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: False
    DEPTHS: [2, 2, 6, 2]
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS: [3, 6, 12, 24]
    PATCH_NORM: True
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  TYPE: swinv2
OUTPUT: /content/drive/MyDrive/jax_exercise/output
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: True
  SEQUENTIAL: False
THROUGHPUT_MODE: False
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: True
  BASE_LR: 0.0008
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: lambda
  MIN_LR: 5e-06
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: False
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5e-07
  WEIGHT_DECAY: 0.0005
/usr/local/lib/python3.7/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Epoch: [0/300] Iter:[0/147], Time: 3.98, lr: [0.0008, 0.0008], Loss: 2.316990
Epoch: [0/300] Iter:[10/147], Time: 0.63, lr: [0.0008, 0.0008], Loss: 2.424551
Epoch: [0/300] Iter:[20/147], Time: 0.47, lr: [0.0008, 0.0008], Loss: 2.324758
Epoch: [0/300] Iter:[30/147], Time: 0.41, lr: [0.0008, 0.0008], Loss: 2.271800
Epoch: [0/300] Iter:[40/147], Time: 0.38, lr: [0.0008, 0.0008], Loss: 2.246047
Epoch: [0/300] Iter:[50/147], Time: 0.36, lr: [0.0008, 0.0008], Loss: 2.223895
Epoch: [0/300] Iter:[60/147], Time: 0.35, lr: [0.0008, 0.0008], Loss: 2.205487
Epoch: [0/300] Iter:[70/147], Time: 0.34, lr: [0.0008, 0.0008], Loss: 2.190410
Epoch: [0/300] Iter:[80/147], Time: 0.34, lr: [0.0008, 0.0008], Loss: 2.176315
Epoch: [0/300] Iter:[90/147], Time: 0.33, lr: [0.0008, 0.0008], Loss: 2.157090
Epoch: [0/300] Iter:[100/147], Time: 0.33, lr: [0.0008, 0.0008], Loss: 2.151388
Epoch: [0/300] Iter:[110/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 2.146101
Epoch: [0/300] Iter:[120/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 2.137084
Epoch: [0/300] Iter:[130/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 2.134636
Epoch: [0/300] Iter:[140/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 2.126191
=> saving checkpoint to /content/drive/MyDrive/jax_exercise/output/imagenet/swint_torch/checkpoint.pth.tar
Loss: 2.011, Mean Accuracy:  0.2652, Best Accuracy:  0.2652
/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch: [1/300] Iter:[0/147], Time: 2.48, lr: [0.0008, 0.0008], Loss: 2.045207
Epoch: [1/300] Iter:[10/147], Time: 0.49, lr: [0.0008, 0.0008], Loss: 1.980358
Epoch: [1/300] Iter:[20/147], Time: 0.40, lr: [0.0008, 0.0008], Loss: 1.985437
Epoch: [1/300] Iter:[30/147], Time: 0.36, lr: [0.0008, 0.0008], Loss: 1.990279
Epoch: [1/300] Iter:[40/147], Time: 0.34, lr: [0.0008, 0.0008], Loss: 1.972596
Epoch: [1/300] Iter:[50/147], Time: 0.33, lr: [0.0008, 0.0008], Loss: 1.976097
Epoch: [1/300] Iter:[60/147], Time: 0.33, lr: [0.0008, 0.0008], Loss: 1.957852
Epoch: [1/300] Iter:[70/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 1.938509
Epoch: [1/300] Iter:[80/147], Time: 0.32, lr: [0.0008, 0.0008], Loss: 1.928319
Epoch: [1/300] Iter:[90/147], Time: 0.31, lr: [0.0008, 0.0008], Loss: 1.921083
Traceback (most recent call last):
  File "main_torch.py", line 185, in <module>
    trainloader, criterion, optimizer, model, writer_dict)
  File "/content/drive/MyDrive/jax_exercise/swin_transformer_jax/core/function.py", line 42, in train_torch
    loss.backward()
  File "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py", line 175, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt